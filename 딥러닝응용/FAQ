인공지능에서 지능에 해당하는 기능
지능에 해당하는 기능은 분류, 예측, 회귀와 같은 작업을 수행하는 능력입니다. 이를 가능하게 하는 능력은 데이터에서 패턴을 학습하고, 이를 바탕으로 새로운 데이터를 예측하는 것입니다.

인공지능의 종류 3가지
지도학습(Supervised Learning): **정답(label)**이 있는 데이터를 통해 학습합니다. 주로 분류와 회귀 작업에 사용됩니다.
비지도학습(Unsupervised Learning): 정답 없이 패턴을 찾는 학습입니다. 주로 군집화와 차원 축소 작업에 사용됩니다.
강화학습(Reinforcement Learning): 보상을 통해 행동을 학습하는 방법으로, 에이전트가 환경과 상호작용하며 최적의 행동을 학습합니다.
전통적인 프로그래밍 방법 vs. 인공지능 프로그램
전통적인 프로그래밍: 프로그래머가 명시적으로 규칙을 정의하여 프로그램을 작성합니다.
인공지능 프로그램: 데이터로부터 스스로 규칙을 학습하여 문제를 해결합니다.
딥러닝 vs. 머신러닝
머신러닝: 데이터를 통해 모델을 학습하는 방법론의 총칭입니다. 특징을 수작업으로 추출하고, 알고리즘을 통해 학습합니다.
딥러닝: 인공신경망을 기반으로 한 머신러닝의 하위 분야로, 데이터를 통해 자동으로 특징을 추출하는 깊은 신경망 구조를 사용합니다.
Classification vs. Regression
분류(Classification): 데이터가 어느 카테고리에 속하는지 예측하는 작업입니다.
회귀(Regression): 연속적인 값을 예측하는 작업입니다.
차원의 저주(Curse of Dimensionality)
데이터의 차원이 높아질수록 모델이 과적합될 가능성이 커지고, 학습이 어려워지는 현상을 말합니다.

Dimensionality Reduction의 필요성
차원을 줄임으로써 과적합을 방지하고, 계산 효율성을 높이며, 데이터의 중요한 특징을 잘 표현할 수 있도록 돕습니다.

Ridge vs. Lasso
공통점: 둘 다 정규화(Regularization) 기법으로, 모델의 복잡도를 줄여 과적합을 방지합니다.
차이점: Ridge는 L2 정규화를 사용해 가중치를 작게 만들고, Lasso는 L1 정규화를 사용해 일부 가중치를 0으로 만들어 특성 선택 효과를 줍니다.
Overfitting vs. Underfitting
Overfitting: 모델이 학습 데이터에 너무 적합하여 새로운 데이터에 일반화되지 못하는 상태.
Underfitting: 모델이 충분히 학습하지 못해 학습 데이터에서도 성능이 떨어지는 상태.
Feature Engineering vs. Feature Selection
Feature Engineering: 새로운 특성을 만들어내는 과정.
Feature Selection: 모델 학습에 필요한 중요한 특성만을 선택하는 과정.
전처리(Preprocessing)의 목적과 방법
목적: 데이터를 깨끗하게 하여 모델이 더 나은 학습을 할 수 있게 준비하는 것.
방법: 노이즈 제거, 이상치 처리, 결측치 대체 등이 있습니다.
EDA(Exploratory Data Analysis)
데이터의 분포, 상관관계 등을 파악하여 데이터의 특성을 이해하는 과정입니다.

회귀에서 절편과 기울기의 의미
절편: 회귀선이 y축을 만나는 값, 데이터의 기준점을 뜻합니다.
기울기: 변수 간의 관계를 나타냅니다. 딥러닝에서 가중치와 연결됩니다.
Activation Function
이유: 신경망에 비선형성을 부여하여 복잡한 패턴을 학습하게 만듭니다.
Softmax: 다중 분류에서 사용, 출력을 확률로 변환합니다.
Sigmoid: 이진 분류에서 사용, 출력을 0~1 사이 값으로 변환합니다.
Forward Propagation vs. Backward Propagation
Forward Propagation: 입력 데이터를 모델을 통해 출력으로 계산하는 과정.
Backward Propagation: 손실을 계산한 후, 기울기를 통해 가중치를 업데이트하는 과정.
손실함수
모델이 예측한 값과 실제 값 사이의 차이를 계산하는 함수입니다.

종류: MSE, MAE, Cross-Entropy, Hinge Loss
옵티마이저(Optimizer)
손실 함수의 값을 최소화하기 위해 가중치 업데이트를 조정하는 알고리즘입니다. 경사 하강법, Adam 등이 있습니다.

경사하강법
손실 함수의 기울기를 사용해 가중치를 업데이트하는 방법.

확률적 경사 하강법: 한 번에 하나의 데이터를 사용해 업데이트.
배치 경사 하강법: 모든 데이터를 사용해 업데이트.
미니 배치 경사 하강법: 소규모 배치를 사용해 업데이트.
교차검증 vs. K-fold 교차검증
교차검증: 데이터를 여러 번 나누어 모델을 평가.
K-fold 교차검증: 데이터를 K개의 부분으로 나눠 K번 학습과 검증을 반복.
하이퍼파라미터 튜닝
모델 학습에 사용되는 하이퍼파라미터(학습률, 에포크 수 등)를 최적화하는 과정.

CNN의 합성곱 역할
이미지의 지역적 특징을 추출하는 역할을 합니다.

CNN의 풀링층 역할
차원을 줄이고, 중요한 정보를 추출하여 계산량을 줄이는 역할을 합니다.

CNN의 Dense Layer 역할
이미지에서 추출된 특징을 기반으로 분류를 수행합니다.

CNN의 stride와 filter
Stride: 필터를 움직이는 간격, 큰 값일수록 출력이 작아짐.
Filter: 이미지의 특징을 추출하는데 사용되며, 가중치는 학습을 통해 결정됩니다.
RNN을 사용하는 이유와 한계
RNN은 순차적 데이터(시계열, 텍스트)를 처리할 수 있지만, 기울기 소실 문제가 있습니다.

LSTM을 사용하는 이유와 한계
LSTM은 RNN의 기울기 소실 문제를 해결하고, 긴 시퀀스를 처리할 수 있습니다. 하지만 계산량이 많습니다.

GRU와 차별성
GRU는 LSTM과 비슷하지만, 구조가 간단하고 학습 속도가 빠릅니다.

결정트리의 불순도(Impurity) – 지니 계수
데이터가 얼마나 섞여 있는지를 나타내는 지표로, 0일수록 순수한 상태입니다.

앙상블
여러 개의 모델을 결합하여 더 나은 성능을 내는 방법입니다.

부트스트래핑
데이터를 중복을 허용하여 여러 번 샘플링하는 방법입니다.

배깅
여러 모델을 학습시켜 결과를 평균내어 예측 성능을 높이는 방법입니다.

주성분 분석(PCA)
고차원 데이터를 저차원으로 변환하여 중요한 특징을 추출하는 방법입니다.

Dense Layer
모든 노드가 서로 완전 연결된 레이어로, 분류나 예측에 사용됩니다.
