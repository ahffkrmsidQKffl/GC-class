241017 7주차 딥러닝응용

[9:01] FCNN = Dense layer -> CNN이 나옴
 둘의 차이점? = 인접한 pixel과 같이 lay?를 추출
 RNN은 뭐가 달라지죠? 현재 정보를 받아들일 뿐 아니라 이전 상태의 정보를 반영 -> 그럼에도 불구하고 GVP를 해결하지 못 함
LSTM

[9:13] ResNet의 기본 컨셉? skip-connection
 장점 1 : f(x)+a로 GVP 해결
 장점 2 : 잔차 학습

ㄴ 우리가 지금 다루는 건 결국 variation of CNN

skip connection에서 파생

1. SE 
 채널 중요도 -> avp
 channel attention

중요한 것에 집중하자 = attention

2. inception : 서로 다른 filter

3. depthwise seperable convolution :

4. CBAM : 채널 어텐션 + 스패셜 어텐션, 주로 여기서 들어가는 레이어는 풀리네어?
 

적을 필요 없다 배리애이션들 깃허브에 다 올려놧다 하시네 공지사항 말하시나

[9:28] 1. transformer (임베딩->벡터임베딩, 포지셔널 임베딩, 셀프어텐션?, ???)

2. 오토 인코더 (사용 목적 : 차원 축소, 

[9:45?] 과목공지에 올린 링크들 들어가면서 확인 하심

[10:21] 과제 설명? 
 0. linear 오토인코더
 1. non-linear AZ
 2. (선택과제) conv LSTM
 3. 잡음 제거 오토인코더 

[10:40] 오토인코더가 뭔지에 대해 다시 설명 (교수님 깃허브에서 AE_credit 들어감)
 
