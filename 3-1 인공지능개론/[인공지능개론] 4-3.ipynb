{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMVaJ/KVmyA9fQr62P1AF7A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":239},"id":"8o-F5oZc1Ayb","executionInfo":{"status":"error","timestamp":1713826830443,"user_tz":-540,"elapsed":460,"user":{"displayName":"강민형","userId":"09960289264787508452"}},"outputId":"17426e6f-ff38-4827-aae9-cabf3c8c06dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["#0번째 폴드 처리중\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'np' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-7a908b80a17b>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_val_samples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_val_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mval_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_val_samples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_val_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     partial_train_data = np.concatenate(\n\u001b[0m\u001b[1;32m     29\u001b[0m         [train_data[:i * num_val_samples],\n\u001b[1;32m     30\u001b[0m          train_data[(i + 1) * num_val_samples:]],\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}],"source":["from tensorflow.keras.datasets import boston_housing\n","(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n","\n","mean = train_data.mean(axis=0)\n","train_data -= mean\n","std = train_data.std(axis=0)\n","train_data /= std\n","test_data -= mean\n","test_data /= std\n","\n","def build_model():\n","    model = keras.Sequential([\n","        layers.Dense(64, activation=\"relu\"),\n","        layers.Dense(64, activation=\"relu\"),\n","        layers.Dense(1)\n","    ])\n","    model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n","    return model\n","\n","k = 4\n","num_val_samples = len(train_data) // k\n","num_epochs = 100\n","all_scores = []\n","for i in range(k):\n","    print(f\"#{i}번째 폴드 처리중\")\n","    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n","    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n","    partial_train_data = np.concatenate(\n","        [train_data[:i * num_val_samples],\n","         train_data[(i + 1) * num_val_samples:]],\n","        axis=0)\n","    partial_train_targets = np.concatenate(\n","        [train_targets[:i * num_val_samples],\n","         train_targets[(i + 1) * num_val_samples:]],\n","        axis=0)\n","    model = build_model()\n","    model.fit(partial_train_data, partial_train_targets,\n","              epochs=num_epochs, batch_size=16, verbose=0)\n","    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n","    all_scores.append(val_mae)\n","\n","num_epochs = 500\n","all_mae_histories = []\n","for i in range(k):\n","    print(f\"#{i}번째 폴드 처리중\")\n","    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n","    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n","    partial_train_data = np.concatenate(\n","        [train_data[:i * num_val_samples],\n","         train_data[(i + 1) * num_val_samples:]],\n","        axis=0)\n","    partial_train_targets = np.concatenate(\n","        [train_targets[:i * num_val_samples],\n","         train_targets[(i + 1) * num_val_samples:]],\n","        axis=0)\n","    model = build_model()\n","    history = model.fit(partial_train_data, partial_train_targets,\n","                        validation_data=(val_data, val_targets),\n","                        epochs=num_epochs, batch_size=16, verbose=0)\n","    mae_history = history.history[\"val_mae\"]\n","    all_mae_histories.append(mae_history)\n","average_mae_history = [\n","    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n","plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Validation MAE\")\n","plt.show()\n","truncated_mae_history = average_mae_history[10:]\n","plt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Validation MAE\")\n","plt.show()\n","\n","model = build_model()\n","model.fit(train_data, train_targets,\n","          epochs=130, batch_size=16, verbose=0)\n","test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n","test_mae_score\n","\n","predictions = model.predict(test_data)\n","predictions[0]"]}]}